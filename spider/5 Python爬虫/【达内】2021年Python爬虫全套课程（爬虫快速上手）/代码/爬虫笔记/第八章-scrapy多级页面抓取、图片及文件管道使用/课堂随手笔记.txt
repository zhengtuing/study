

一级页面：
     a_list ： //li[contains(@id, "menu-item-")]/a

     1、大标题的名称  :  ./text()
     2、大标题的链接  :  ./@href
   
二级页面：
    a_list  :  //article/a

     1、小标题的名称  :  ./text()
     2、小标题的链接  :  ./@href

三级页面：
      结果： //article/p/text()    ---->    ['段落1', '段落2', '段落3', '段落4', '段落5']
     1、具体小说内容
     comment :   '\n'.join(['段落1', '段落2', '段落3', '段落4', '段落5'])   
                         '段落1\n段落2\n段落3\n段落4\段落5'

     





GET请求：yield scrapy.Request(url=url, meta={}, callback=...)

POST请求：yield scrapy.FormRequest(url=url, formdata={}, meta={}, callback=...)


方法名：FormRequest
参数名：fromdata
一定要重写start_requests()方法



1、爬虫文件中,将图片的链接提取出来直接交给管道文件处理
2、管道文件中,导入并继承scrapy的ImagesPipeline类
   重写 get_media_requests()方法 和 file_path() 方法
3、全局配置文件中,通过IMAGES_STORE变量指定图片保存的位置




1、一级页面URL地址：http://www.1ppt.com/xiazai/
   提取数据(29个分类)：
      li_list = '//div[@class="col_nav i_nav clearfix"]/ul[3]/li'
      1.1> 栏目分类名称 ： ./a/text()
      1.2> 栏目分类链接 ： ./a/@href    
                           需要和 http://www.1ppt.com 进行拼接

2、二级页面：进入到了某个栏目分类
   提取数据(20个PPT)：
      li_list = '//ul[@class="tplist"]/li'
      2.1> PPT名称 ：./h2/a/text()
      2.2> 进入PPT详情页链接 ： ./h2/a/@href  
                                需要和 http://www.1ppt.com 进行拼接

3、三级页面：进入到了PPT的详情页
   提取数据：
      3.1> 进入下载页的链接 : //ul[@class="downurllist"]/li/a/@href
                              需要和 http://www.1ppt.com 进行拼接

4、四级页面
   提取数据：
      4.1> 具体PPT的下载链接 ---> 交给项目管道文件处理
           //ul[@class="downloadlist"]/li[1]/a/@href


案例需要注意：
1、四级页面: 注意meta参数的使用
2、文件：注意FilesPipeline管道的使用











